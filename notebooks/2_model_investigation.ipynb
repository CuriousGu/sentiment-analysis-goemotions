{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bmarquescost\\anaconda3\\envs\\sa-data\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import evaluate\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import BertForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH  = 512\n",
    "TRAIN_RATIO = 0.7\n",
    "VAL_RATIO   = 0.2\n",
    "TEST_RATIO  = 0.1\n",
    "BATCH_SIZE  = 16 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device=device(type='cuda', index=0)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'{device=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../go_emotions_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 id\n",
      "1 text\n",
      "2 example_very_unclear\n",
      "3 admiration\n",
      "4 amusement\n",
      "5 anger\n",
      "6 annoyance\n",
      "7 approval\n",
      "8 caring\n",
      "9 confusion\n",
      "10 curiosity\n",
      "11 desire\n",
      "12 disappointment\n",
      "13 disapproval\n",
      "14 disgust\n",
      "15 embarrassment\n",
      "16 excitement\n",
      "17 fear\n",
      "18 gratitude\n",
      "19 grief\n",
      "20 joy\n",
      "21 love\n",
      "22 nervousness\n",
      "23 optimism\n",
      "24 pride\n",
      "25 realization\n",
      "26 relief\n",
      "27 remorse\n",
      "28 sadness\n",
      "29 surprise\n",
      "30 neutral\n"
     ]
    }
   ],
   "source": [
    "for i, col in enumerate(list(df.columns)):\n",
    "    print(i, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = {\n",
    "    '0': ['excitement', 'joy', 'gratitude'],\n",
    "    '1': ['admiration', 'approval', 'optimism'],\n",
    "    '2': ['amusement', 'caring', 'desire', 'love'],\n",
    "    '3': ['curiosity', 'realization', 'surprise'],\n",
    "    '4': ['anger', 'nervousness'],\n",
    "    '5': ['annoyance', 'confusion', 'remorse'],\n",
    "    '6': ['disappointment', 'disapproval', 'disgust', 'embarrassment'],\n",
    "    '7': ['sadness', 'grief'],\n",
    "    '8': ['fear']\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_to_id = {v: k for k, value in classes.items() for v in value}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = class_to_id.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(211225, 31)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(35199, 31)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df[df[columns].sum(axis=1) > 1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existem 35199 amostras com mais de uma class, mas 211225 amostras totais, o que resulta em 176.026 amostras com clase única.\n",
    "\n",
    "Podemos não dropar, mas sim duplicar as amostras com mais de uma label (isso seria interessante para o caso em que elementos estão na mesma classe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = df[df[columns].sum(axis=1) > 1].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'] = df.reset_index()['index'].map(df[columns].idxmax(1).to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['class'] = df['label'].map(class_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>example_very_unclear</th>\n",
       "      <th>admiration</th>\n",
       "      <th>amusement</th>\n",
       "      <th>anger</th>\n",
       "      <th>annoyance</th>\n",
       "      <th>approval</th>\n",
       "      <th>caring</th>\n",
       "      <th>confusion</th>\n",
       "      <th>...</th>\n",
       "      <th>optimism</th>\n",
       "      <th>pride</th>\n",
       "      <th>realization</th>\n",
       "      <th>relief</th>\n",
       "      <th>remorse</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>neutral</th>\n",
       "      <th>label</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>eew5j0j</td>\n",
       "      <td>That game hurt.</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>sadness</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eemcysk</td>\n",
       "      <td>&gt;sexuality shouldn’t be a grouping category I...</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>excitement</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ed2mah1</td>\n",
       "      <td>You do right, if you don't care then fuck 'em!</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>excitement</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>eeibobj</td>\n",
       "      <td>Man I love reddit.</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>love</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>eda6yn6</td>\n",
       "      <td>[NAME] was nowhere near them, he was by the Fa...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>excitement</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text  \\\n",
       "0  eew5j0j                                    That game hurt.   \n",
       "1  eemcysk   >sexuality shouldn’t be a grouping category I...   \n",
       "2  ed2mah1     You do right, if you don't care then fuck 'em!   \n",
       "3  eeibobj                                 Man I love reddit.   \n",
       "4  eda6yn6  [NAME] was nowhere near them, he was by the Fa...   \n",
       "\n",
       "   example_very_unclear  admiration  amusement  anger  annoyance  approval  \\\n",
       "0                 False           0          0      0          0         0   \n",
       "1                  True           0          0      0          0         0   \n",
       "2                 False           0          0      0          0         0   \n",
       "3                 False           0          0      0          0         0   \n",
       "4                 False           0          0      0          0         0   \n",
       "\n",
       "   caring  confusion  ...  optimism  pride  realization  relief  remorse  \\\n",
       "0       0          0  ...         0      0            0       0        0   \n",
       "1       0          0  ...         0      0            0       0        0   \n",
       "2       0          0  ...         0      0            0       0        0   \n",
       "3       0          0  ...         0      0            0       0        0   \n",
       "4       0          0  ...         0      0            0       0        0   \n",
       "\n",
       "   sadness  surprise  neutral       label  class  \n",
       "0        1         0        0     sadness      7  \n",
       "1        0         0        0  excitement      0  \n",
       "2        0         0        1  excitement      0  \n",
       "3        0         0        0        love      2  \n",
       "4        0         0        1  excitement      0  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pré-processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labeled = df[['text', 'class']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_text(text):\n",
    "    preprocessed_text = re.sub(r'\\n+', '\\n', text)\n",
    "    preprocessed_text = re.sub(r'http\\S+', '', text) # removendo links\n",
    "    preprocessed_text = preprocessed_text.replace('\"', '')    # removendo aspas\n",
    "    preprocessed_text = re.sub(r\"<\\S*\\ ?\\/?>\", '', preprocessed_text)\n",
    "    # preprocessed_text = re.sub(\"[-*!,$><:.+?=]\", '', preprocessed_text) # remove outras pontuações\n",
    "\n",
    "    # preprocessed_text = re.sub(r'[.]\\s+', '', preprocessed_text)  # removendo reticências \n",
    "    preprocessed_text = re.sub(r'  ', ' ', preprocessed_text) # removendo espaços extras\n",
    "    preprocessed_text = re.sub(r'\\'', \"''\", preprocessed_text)\n",
    "    return preprocessed_text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bmarquescost\\AppData\\Local\\Temp\\ipykernel_12676\\1313342462.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_labeled['preprocessed_text'] = df_labeled['text'].apply(pre_process_text, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "      <th>preprocessed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>That game hurt.</td>\n",
       "      <td>7</td>\n",
       "      <td>that game hurt.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&gt;sexuality shouldn’t be a grouping category I...</td>\n",
       "      <td>0</td>\n",
       "      <td>&gt;sexuality shouldn’t be a grouping category i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>You do right, if you don't care then fuck 'em!</td>\n",
       "      <td>0</td>\n",
       "      <td>you do right, if you don''t care then fuck ''em!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Man I love reddit.</td>\n",
       "      <td>2</td>\n",
       "      <td>man i love reddit.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[NAME] was nowhere near them, he was by the Fa...</td>\n",
       "      <td>0</td>\n",
       "      <td>[name] was nowhere near them, he was by the fa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text class  \\\n",
       "0                                    That game hurt.     7   \n",
       "1   >sexuality shouldn’t be a grouping category I...     0   \n",
       "2     You do right, if you don't care then fuck 'em!     0   \n",
       "3                                 Man I love reddit.     2   \n",
       "4  [NAME] was nowhere near them, he was by the Fa...     0   \n",
       "\n",
       "                                   preprocessed_text  \n",
       "0                                    that game hurt.  \n",
       "1   >sexuality shouldn’t be a grouping category i...  \n",
       "2   you do right, if you don''t care then fuck ''em!  \n",
       "3                                 man i love reddit.  \n",
       "4  [name] was nowhere near them, he was by the fa...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_labeled['preprocessed_text'] = df_labeled['text'].apply(pre_process_text, 1)\n",
    "df_labeled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregando modelo e tokenizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = 'distilbert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tokenized = tokenizer(df_labeled['preprocessed_text'].to_list(), return_tensors='pt', padding=True, truncation=True, max_length=MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([211225, 316]) torch.Size([211225, 316])\n"
     ]
    }
   ],
   "source": [
    "print(df_tokenized['input_ids'].shape, df_tokenized['attention_mask'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    load_accuracy = evaluate.load('accuracy')\n",
    "\n",
    "    logits, labels = eval_pred\n",
    "\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    accuracy = load_accuracy.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
    "    return {\"accuracy\": accuracy}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.len = len(y)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.X.items()} \n",
    "        item['class'] = self.y[idx]\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['7', '0', '0', ..., '1', '4', '0'], dtype=object)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_labeled['class'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TextDataset(df_tokenized, torch.tensor(df_labeled['class'].astype('int').to_numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino: 147858, Val.: 42245, Teste: 21122\n"
     ]
    }
   ],
   "source": [
    "n_train_instances = int(np.round(dataset.len * TRAIN_RATIO))\n",
    "n_val_instances = int(np.round(dataset.len * VAL_RATIO))\n",
    "n_test_instances = int(np.round(dataset.len * TEST_RATIO))\n",
    "print(f'Treino: {n_train_instances}, Val.: {n_val_instances}, Teste: {n_test_instances}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split, val_split, test_split = torch.utils.data.random_split(dataset, [n_train_instances, n_val_instances, n_test_instances])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinando com training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=9, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = 'distilbert-base-uncased'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=len(classes.keys()))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_split, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_split, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_split, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "steps_per_epoch = 200\n",
    "epoch_validation_samples = 50\n",
    "learning_rate = 2e-5\n",
    "\n",
    "optim = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_step(model, batch_data, cur_step, compute_evaluation=False, optimizer=None):\n",
    "    if cur_step == 'train':\n",
    "        model.train()\n",
    "    elif cur_step == 'val':\n",
    "        model.eval()\n",
    "    \n",
    "    input_ids = batch_data['input_ids'].to(device)\n",
    "    attention_mask = batch_data['attention_mask'].to(device)\n",
    "    labels = batch_data['class'].to(device)\n",
    "\n",
    "    output = model(input_ids, attention_mask=attention_mask, labels=labels.long())\n",
    "    \n",
    "    loss = output.loss\n",
    "    logits = output.logits\n",
    "\n",
    "    if cur_step == 'train':\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    evaluation = None\n",
    "    if compute_evaluation:\n",
    "        softmax_predictions = torch.nn.functional.softmax(logits, dim=1)\n",
    "        evaluation = compute_metrics([softmax_predictions.detach().cpu(), labels])\n",
    "\n",
    "    return loss.item() * labels.shape[0], evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 50/50 [53:07<00:00,  1.14samples/s]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch summary [1/5]\t Train loss: 1.383182343720832\t Train acc: 0.4722222222222222\t Val loss: 0.024501521850650954\t Val acc: 0.5225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 50/50 [52:21<00:00,  1.13samples/s]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch summary [2/5]\t Train loss: 1.2709584075186922\t Train acc: 0.4895833333333333\t Val loss: 0.02590853598205588\t Val acc: 0.4925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 50/50 [52:17<00:00,  1.17samples/s]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch summary [3/5]\t Train loss: 1.1902091727762256\t Train acc: 0.5347222222222222\t Val loss: 0.02478266149487689\t Val acc: 0.5025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 50/50 [52:14<00:00,  1.18samples/s]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch summary [4/5]\t Train loss: 1.1116256486668268\t Train acc: 0.4895833333333333\t Val loss: 0.026169311874702086\t Val acc: 0.50375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 50/50 [52:15<00:00,  1.17samples/s]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch summary [5/5]\t Train loss: 1.038097089735516\t Train acc: 0.5694444444444444\t Val loss: 0.02846104448851022\t Val acc: 0.47875\n"
     ]
    }
   ],
   "source": [
    "epoch_data = {}\n",
    "\n",
    "for i in range(epochs):\n",
    "    epoch_data[i] = {'train': [], 'validation': []}\n",
    "    num_train_examples = 0\n",
    "    num_val_examples = 0\n",
    "\n",
    "    train_hits = 0\n",
    "    val_hits = 0\n",
    "\n",
    "    train_bar = tqdm(total=len(train_loader), desc=f\"Train\", unit=\"steps\", position=0, leave=False)\n",
    "    val_bar   = tqdm(total=epoch_validation_samples, desc=f\"Validation\", unit=\"samples\", position=0, leave=False)\n",
    "\n",
    "    train_running_loss = 0\n",
    "    for batch_id, batch_data in enumerate(train_loader):\n",
    "        if (batch_id + 1) % 500 == 0:\n",
    "            loss, evaluation = model_step(model, batch_data, 'train', True, optim)\n",
    "            epoch_data[i]['train'].append(evaluation)\n",
    "        else:\n",
    "            loss, _ = model_step(model, batch_data, 'train', False, optim)\n",
    "\n",
    "        train_running_loss += loss\n",
    "\n",
    "        train_bar.update(1)\n",
    "\n",
    "    val_running_loss = 0\n",
    "    for batch_id, batch_data in enumerate(val_loader):\n",
    "        loss, evaluation = model_step(model, batch_data, 'val', True)\n",
    "        \n",
    "        val_running_loss += loss\n",
    "\n",
    "        epoch_data[i]['validation'].append(evaluation)\n",
    "        \n",
    "        val_bar.update(1)\n",
    "\n",
    "        if (batch_id + 1) % epoch_validation_samples == 0:\n",
    "            break\n",
    "    \n",
    "    train_acc = np.mean([eval['accuracy'] for eval in epoch_data[i]['train']])\n",
    "    val_acc = np.mean([eval['accuracy'] for eval in epoch_data[i]['validation']])\n",
    "    \n",
    "    train_loss = train_running_loss / len(train_loader.sampler)\n",
    "    valid_loss = val_running_loss / len(val_loader.sampler)\n",
    "\n",
    "    print(f\"Epoch summary [{i+1}/{epochs}]\\t Train loss: {train_loss}\\t Train acc: {train_acc}\\t Val loss: {valid_loss}\\t Val acc: {val_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './model_weights_loop.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sa-data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
